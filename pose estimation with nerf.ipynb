{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the good stuff\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meshgrid_xy(tensor1: torch.Tensor, tensor2: torch.Tensor) -> (torch.Tensor, torch.Tensor):\n",
    "  \n",
    "    # TESTED\n",
    "    ii, jj = torch.meshgrid(tensor1, tensor2)\n",
    "    return ii.transpose(-1, -2), jj.transpose(-1, -2)\n",
    "\n",
    "\n",
    "def cumprod_exclusive(tensor: torch.Tensor) -> torch.Tensor:\n",
    "  # Only works for the last dimension (dim=-1)\n",
    "  dim = -1\n",
    "  # Compute regular cumprod first (this is equivalent to `tf.math.cumprod(..., exclusive=False)`).\n",
    "  cumprod = torch.cumprod(tensor, dim)\n",
    "  # \"Roll\" the elements along dimension 'dim' by 1 element.\n",
    "  cumprod = torch.roll(cumprod, 1, dim)\n",
    "  # Replace the first element by \"1\" as this is what tf.cumprod(..., exclusive=True) does.\n",
    "  cumprod[..., 0] = 1.\n",
    "\n",
    "  return cumprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ray_bundle(height: int, width: int, focal_length: float, tform_cam2world: torch.Tensor):\n",
    "  ii, jj = meshgrid_xy(\n",
    "      torch.arange(width).to(tform_cam2world),\n",
    "      torch.arange(height).to(tform_cam2world)\n",
    "  )\n",
    "  directions = torch.stack([(ii - width * .5) / focal_length,\n",
    "                            -(jj - height * .5) / focal_length,\n",
    "                            -torch.ones_like(ii)\n",
    "                           ], dim=-1)\n",
    "\n",
    "  ray_directions = torch.sum(directions[..., None, :] * tform_cam2world[:3, :3], dim=-1)\n",
    "  ray_origins = tform_cam2world[:3, -1].expand(ray_directions.shape)\n",
    "  return ray_origins, ray_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_query_points_from_rays(\n",
    "    ray_origins: torch.Tensor,\n",
    "    ray_directions: torch.Tensor,\n",
    "    near_thresh: float,\n",
    "    far_thresh: float,\n",
    "    num_samples: int,\n",
    "    randomize: Optional[bool] = True\n",
    ") -> (torch.Tensor, torch.Tensor):\n",
    "\n",
    "  # shape: (num_samples)\n",
    "  depth_values = torch.linspace(near_thresh, far_thresh, num_samples).to(ray_origins)\n",
    "  if randomize is True:\n",
    "    # ray_origins: (width, height, 3)\n",
    "    # noise_shape = (width, height, num_samples)\n",
    "    noise_shape = list(ray_origins.shape[:-1]) + [num_samples]\n",
    "    # depth_values: (num_samples)\n",
    "    depth_values = depth_values \\\n",
    "        + torch.rand(noise_shape).to(ray_origins) * (far_thresh\n",
    "            - near_thresh) / num_samples\n",
    "  # (width, height, num_samples, 3) = (width, height, 1, 3) + (width, height, 1, 3) * (num_samples, 1)\n",
    "  # query_points:  (width, height, num_samples, 3)\n",
    "  query_points = ray_origins[..., None, :] + ray_directions[..., None, :] * depth_values[..., :, None]\n",
    "  # TODO: Double-check that `depth_values` returned is of shape `(num_samples)`.\n",
    "  return query_points, depth_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_volume_density(\n",
    "    radiance_field: torch.Tensor,\n",
    "    ray_origins: torch.Tensor,\n",
    "    depth_values: torch.Tensor\n",
    ") -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "\n",
    "  sigma_a = torch.nn.functional.relu(radiance_field[..., 3])\n",
    "  rgb = torch.sigmoid(radiance_field[..., :3])\n",
    "  one_e_10 = torch.tensor([1e10], dtype=ray_origins.dtype, device=ray_origins.device)\n",
    "  dists = torch.cat((depth_values[..., 1:] - depth_values[..., :-1],\n",
    "                  one_e_10.expand(depth_values[..., :1].shape)), dim=-1)\n",
    "  alpha = 1. - torch.exp(-sigma_a * dists)\n",
    "  weights = alpha * cumprod_exclusive(1. - alpha + 1e-10)\n",
    "\n",
    "  rgb_map = (weights[..., None] * rgb).sum(dim=-2)\n",
    "  depth_map = (weights * depth_values).sum(dim=-1)\n",
    "  acc_map = weights.sum(-1)\n",
    "\n",
    "  return rgb_map, depth_map, acc_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(\n",
    "    tensor, num_encoding_functions=6, include_input=True, log_sampling=True\n",
    ") -> torch.Tensor:\n",
    "\n",
    "  # Trivially, the input tensor is added to the positional encoding.\n",
    "  encoding = [tensor] if include_input else []\n",
    "  # Now, encode the input using a set of high-frequency functions and append the\n",
    "  # resulting values to the encoding.\n",
    "  frequency_bands = None\n",
    "  if log_sampling:\n",
    "      frequency_bands = 2.0 ** torch.linspace(\n",
    "            0.0,\n",
    "            num_encoding_functions - 1,\n",
    "            num_encoding_functions,\n",
    "            dtype=tensor.dtype,\n",
    "            device=tensor.device,\n",
    "        )\n",
    "  else:\n",
    "      frequency_bands = torch.linspace(\n",
    "          2.0 ** 0.0,\n",
    "          2.0 ** (num_encoding_functions - 1),\n",
    "          num_encoding_functions,\n",
    "          dtype=tensor.dtype,\n",
    "          device=tensor.device,\n",
    "      )\n",
    "\n",
    "  for freq in frequency_bands:\n",
    "      for func in [torch.sin, torch.cos]:\n",
    "          encoding.append(func(tensor * freq))\n",
    "\n",
    "  # Special case, for no positional encoding\n",
    "  if len(encoding) == 1:\n",
    "      return encoding[0]\n",
    "  else:\n",
    "      return torch.cat(encoding, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VeryTinyNerfModel(torch.nn.Module):\n",
    "  r\"\"\"Define a \"very tiny\" NeRF model comprising three fully connected layers.\n",
    "  \"\"\"\n",
    "  def __init__(self, filter_size=128, num_encoding_functions=6):\n",
    "    super(VeryTinyNerfModel, self).__init__()\n",
    "    # Input layer (default: 39 -> 128)\n",
    "    self.layer1 = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "    # Layer 2 (default: 128 -> 128)\n",
    "    self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "    # Layer 3 (default: 128 -> 4)\n",
    "    self.layer3 = torch.nn.Linear(filter_size, 4)\n",
    "    # Short hand for torch.nn.functional.relu\n",
    "    self.relu = torch.nn.functional.relu\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.relu(self.layer1(x))\n",
    "    x = self.relu(self.layer2(x))\n",
    "    x = self.layer3(x)\n",
    "    return x\n",
    "\n",
    "class EnhancedNerfModel(torch.nn.Module):\n",
    "    \"\"\"Define an enhanced NeRF model with additional layers and increased filter size for better learning capacity.\"\"\"\n",
    "    def __init__(self, filter_size=256, num_encoding_functions=6):\n",
    "        super(EnhancedNerfModel, self).__init__()\n",
    "        # Input layer: Encoded positions are expanded with positional encoding\n",
    "        self.input_layer = torch.nn.Linear(3 + 3 * 2 * num_encoding_functions, filter_size)\n",
    "        \n",
    "        # Adding more layers to increase model complexity\n",
    "        self.layer1 = torch.nn.Linear(filter_size, filter_size)\n",
    "        self.layer2 = torch.nn.Linear(filter_size, filter_size)\n",
    "        self.layer3 = torch.nn.Linear(filter_size, filter_size)  # Additional layer\n",
    "        self.layer4 = torch.nn.Linear(filter_size, filter_size)  # Additional layer\n",
    "        \n",
    "        # Output layer: Produces RGB and density (sigma)\n",
    "        self.output_layer = torch.nn.Linear(filter_size, 4)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.input_layer(x))\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.relu(self.layer2(x))\n",
    "        x = self.relu(self.layer3(x))  # Additional ReLU activation for new layers\n",
    "        x = self.relu(self.layer4(x))  # Additional ReLU activation for new layers\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_minibatches(inputs: torch.Tensor, chunksize: Optional[int] = 1024 * 8):\n",
    "\n",
    "  return [inputs[i:i + chunksize] for i in range(0, inputs.shape[0], chunksize)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample data used in the official tiny_nerf example\n",
    "import os\n",
    "if not os.path.exists('tiny_nerf_data.npz'):\n",
    "    !wget http://cseweb.ucsd.edu/~viscomp/projects/LF/papers/ECCV20/nerf/tiny_nerf_data.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"tiny_nerf_data.npz\")\n",
    "\n",
    "# Images\n",
    "images = data[\"images\"]\n",
    "# Camera extrinsics (poses)\n",
    "tform_cam2world = data[\"poses\"]\n",
    "tform_cam2world = torch.from_numpy(tform_cam2world).to(device)\n",
    "# Focal length (intrinsics)\n",
    "focal_length = data[\"focal\"]\n",
    "focal_length = torch.from_numpy(focal_length).to(device)\n",
    "\n",
    "# Height and width of each image\n",
    "height, width = images.shape[1:3]\n",
    "\n",
    "# Near and far clipping thresholds for depth values.\n",
    "near_thresh = 2.\n",
    "far_thresh = 6.\n",
    "\n",
    "test_image_idx = 99 # test image for evaluation\n",
    "\n",
    "# Hold one image out (for test).\n",
    "testimg, testpose = images[test_image_idx], tform_cam2world[test_image_idx]\n",
    "testimg = torch.from_numpy(testimg).to(device)\n",
    "\n",
    "# Map images to device\n",
    "images = torch.from_numpy(images[:100, ..., :3]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(testimg.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One iteration of TinyNeRF (forward pass).\n",
    "def run_one_iter_of_tinynerf(height, width, focal_length, tform_cam2world,\n",
    "                             near_thresh, far_thresh, depth_samples_per_ray,\n",
    "                             encoding_function, get_minibatches_function):\n",
    "\n",
    "  # Get the \"bundle\" of rays through all image pixels.\n",
    "  ray_origins, ray_directions = get_ray_bundle(height, width, focal_length,\n",
    "                                               tform_cam2world)\n",
    "\n",
    "  # Sample query points along each ray\n",
    "  query_points, depth_values = compute_query_points_from_rays(\n",
    "      ray_origins, ray_directions, near_thresh, far_thresh, depth_samples_per_ray\n",
    "  )\n",
    "\n",
    "  # \"Flatten\" the query points.\n",
    "  flattened_query_points = query_points.reshape((-1, 3))\n",
    "\n",
    "  # Encode the query points (default: positional encoding).\n",
    "  encoded_points = encoding_function(flattened_query_points)\n",
    "\n",
    "  # Split the encoded points into \"chunks\", run the model on all chunks, and\n",
    "  # concatenate the results (to avoid out-of-memory issues).\n",
    "  batches = get_minibatches_function(encoded_points, chunksize=chunksize)\n",
    "  predictions = []\n",
    "  for batch in batches:\n",
    "    predictions.append(model(batch))\n",
    "  radiance_field_flattened = torch.cat(predictions, dim=0)\n",
    "\n",
    "  # \"Unflatten\" to obtain the radiance field.\n",
    "  unflattened_shape = list(query_points.shape[:-1]) + [4]\n",
    "  radiance_field = torch.reshape(radiance_field_flattened, unflattened_shape)\n",
    "\n",
    "  # Perform differentiable volume rendering to re-synthesize the RGB image.\n",
    "  rgb_predicted, _, _ = render_volume_density(radiance_field, ray_origins, depth_values)\n",
    "\n",
    "  return rgb_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters for TinyNeRF training\n",
    "\"\"\"\n",
    "\n",
    "# Number of functions used in the positional encoding \n",
    "num_encoding_functions = 6\n",
    "# Specify encoding function.\n",
    "encode = lambda x: positional_encoding(x, num_encoding_functions=num_encoding_functions)\n",
    "# Number of depth samples along each ray.\n",
    "depth_samples_per_ray = 32\n",
    "\n",
    "# Chunksize\n",
    "chunksize = 16384 \n",
    "test_image_idx = 97 # test image for evaluation\n",
    "\n",
    "# Optimizer parameters\n",
    "lr = 1e-4\n",
    "num_iters = 10000\n",
    "\n",
    "\n",
    "# Misc parameters\n",
    "display_every = 200  # Number of iters after which stats are displayed\n",
    "\n",
    "\"\"\"\n",
    "Model\n",
    "\"\"\"\n",
    "#model = VeryTinyNerfModel(num_encoding_functions=num_encoding_functions)\n",
    "#model.to(device)\n",
    "\n",
    "model = EnhancedNerfModel(num_encoding_functions=num_encoding_functions)\n",
    "model.to(device)\n",
    "\n",
    "\"\"\"\n",
    "Optimizer\n",
    "\"\"\"\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\"\"\"\n",
    "Train-Eval-Repeat!\n",
    "\"\"\"\n",
    "\n",
    "# Seed RNG, for repeatability\n",
    "seed = 9458\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Lists to log metrics etc.\n",
    "psnrs = []\n",
    "iternums = []\n",
    "\n",
    "for i in range(num_iters):\n",
    "\n",
    "  # Randomly pick an image as the target.\n",
    "  target_img_idx = np.random.randint(images.shape[0])\n",
    "  target_img = images[target_img_idx].to(device)\n",
    "  target_tform_cam2world = tform_cam2world[target_img_idx].to(device)\n",
    "\n",
    "  # Run one iteration of TinyNeRF and get the rendered RGB image.\n",
    "  rgb_predicted = run_one_iter_of_tinynerf(height, width, focal_length,\n",
    "                                           target_tform_cam2world, near_thresh,\n",
    "                                           far_thresh, depth_samples_per_ray,\n",
    "                                           encode, get_minibatches)\n",
    "\n",
    "  # Compute mean-squared error between the predicted and target images. Backprop!\n",
    "  loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "\n",
    "  # Display images/plots/stats\n",
    "  if i % display_every == 0:\n",
    "    # Render the held-out view\n",
    "    rgb_predicted = run_one_iter_of_tinynerf(height, width, focal_length,\n",
    "                                             testpose, near_thresh,\n",
    "                                             far_thresh, depth_samples_per_ray,\n",
    "                                             encode, get_minibatches)\n",
    "    loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "    print(\"Loss:\", loss.item())\n",
    "    psnr = -10. * torch.log10(loss)\n",
    "\n",
    "    psnrs.append(psnr.item())\n",
    "    iternums.append(i)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(rgb_predicted.detach().cpu().numpy())\n",
    "    plt.title(f\"Iteration {i}\")\n",
    "    plt.subplot(122)\n",
    "    plt.plot(iternums, psnrs)\n",
    "    plt.title(\"PSNR\")\n",
    "    plt.show()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_comparison(original_img, rendered_img, title1='Original Image', title2='Rendered Image from Estimated Pose'):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(original_img.detach().cpu().numpy())\n",
    "    plt.title(title1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(rendered_img.detach().cpu().numpy())\n",
    "    plt.title(title2)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_curve(losses, best_loss_idx):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Loss')\n",
    "    plt.axvline(x=best_loss_idx, color='r', linestyle='--', label='Best Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def visualize_pose_estimation(target_img, initial_pose, best_pose, model, encode, get_minibatches, height, width, focal_length, near_thresh, far_thresh, depth_samples_per_ray):\n",
    "    # Render images using initial and best poses\n",
    "    rgb_initial = run_one_iter_of_tinynerf(height, width, focal_length, initial_pose,\n",
    "                                           near_thresh, far_thresh, depth_samples_per_ray,\n",
    "                                           encode, get_minibatches)\n",
    "    \n",
    "    rgb_best = run_one_iter_of_tinynerf(height, width, focal_length, best_pose,\n",
    "                                        near_thresh, far_thresh, depth_samples_per_ray,\n",
    "                                        encode, get_minibatches)\n",
    "    \n",
    "    # Plot the target image, initial rendered image, and best rendered image\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(target_img.detach().cpu().numpy())\n",
    "    plt.title('Target Image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(132)\n",
    "    plt.imshow(rgb_initial.detach().cpu().numpy())\n",
    "    plt.title('Rendered Image from Initial Pose')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(133)\n",
    "    plt.imshow(rgb_best.detach().cpu().numpy())\n",
    "    plt.title('Rendered Image from Best Pose')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# pose estimation and visualization\n",
    "initial_img_idx = 5  # Index for the initial guess image\n",
    "target_img_idx = 10   # Index for the target image\n",
    "\n",
    "initial_img = images[initial_img_idx].to(device)\n",
    "initial_pose = tform_cam2world[initial_img_idx].clone()\n",
    "\n",
    "target_img = images[target_img_idx].to(device)\n",
    "target_pose = tform_cam2world[target_img_idx].clone()  # Not used directly in estimation, just for reference\n",
    "\n",
    "# Estimate the pose by comparing the NeRF rendering from `initial_pose` to `target_img`\n",
    "losses = []\n",
    "best_loss = float('inf')\n",
    "best_pose = None\n",
    "best_loss_idx = -1\n",
    "iters = 1000\n",
    "def estimate_pose_with_early_stopping(target_img, initial_pose, model, encode, get_minibatches, height, width, focal_length, near_thresh, far_thresh, depth_samples_per_ray, num_iters=200, lr=0.01, patience=50):\n",
    "    global best_loss, best_pose, best_loss_idx\n",
    "    pose = initial_pose.clone().detach().requires_grad_(True)\n",
    "    optimizer = torch.optim.Adam([pose], lr=lr)\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for i in range(iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Render the image using the current pose\n",
    "        rgb_predicted = run_one_iter_of_tinynerf(height, width, focal_length, pose,\n",
    "                                                 near_thresh, far_thresh, depth_samples_per_ray,\n",
    "                                                 encode, get_minibatches)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = torch.nn.functional.mse_loss(rgb_predicted, target_img)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Check for improvement\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_pose = pose.clone().detach()\n",
    "            best_loss_idx = i\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i} with best loss {best_loss}\")\n",
    "            break\n",
    "        \n",
    "        # Backpropagate the loss and update the pose\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Print and visualize the intermediate results every 50 iterations\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item()}\")\n",
    "            visualize_comparison(target_img, rgb_predicted, title1='Target Image', title2=f'Rendered Image at Iteration {i}')\n",
    "    \n",
    "    return best_pose\n",
    "\n",
    "estimated_pose = estimate_pose_with_early_stopping(target_img, initial_pose, model, encode, get_minibatches, height, width, focal_length, near_thresh, far_thresh, depth_samples_per_ray, num_iters=200)\n",
    "\n",
    "# Visualize the final comparison between the target image and the rendered image from the estimated pose\n",
    "visualize_comparison(target_img, run_one_iter_of_tinynerf(height, width, focal_length, estimated_pose, near_thresh, far_thresh, depth_samples_per_ray, encode, get_minibatches))\n",
    "\n",
    "# Visualize the entire process\n",
    "visualize_pose_estimation(target_img, initial_pose, estimated_pose, model, encode, get_minibatches, height, width, focal_length, near_thresh, far_thresh, depth_samples_per_ray)\n",
    "\n",
    "# Plot the loss curve with the best loss indicated\n",
    "plot_loss_curve(losses, best_loss_idx)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
